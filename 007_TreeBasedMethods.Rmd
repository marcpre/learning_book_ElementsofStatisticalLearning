---
title: "007_TreeBasedMethods"
output: html_document
---

# Tree Based Methods

We basically use stratifying or segmenting predictors space into simple regions, typically using the mean or the mode. Such methods are called decision tree methods. 

## Regression Trees

Are used to predict quantitative responses.

### Prediction via Stratification of the Feature Space

The process for regression trees is as the following:

1. We divide the predictor space into non-overlapping regions

2. For every observation we that falls into one region, we make the same prediction. Simply the mean of the response values.

To create the regions we simply create high-dimensional rectangles. (boxes) We seperate these boxes by minimizing the RSS. As it is impossible to find every box, we simply use recursive binary splitting. 

### Tree Pruning

The above process is likely to overfit the data on the test set, as the resulting fit might be too complex. A tree that is smaller leads to better variance, with a small bias. 

We can obtain an "optimal" tree by growing a very large tree and then *prune* it back in order to obtain a *subtree*. 

How do we determine the best way to prune the tree? Basically we want a subtree with the lowest test error rate. 

Weakest link pruning (Cost complexity pruning) is a viable way to do this. 

## Classification Trees

Classification Trees are used to predict a qualitative response rather than a quantitative one. With a classification tree we predict that each response belongs to the most commonly occurring class. 

Either the Gini index or the cross-entropy can be used for tree growing. However also the classification error rate can be used if global accuracy is needed. 

## Tree versus Linear Models

Linear regression assums the following model:

$$
f(X) = \beta_0 + \sum^p_{j=1} X_j \beta_j
$$

Tree based models assume the following form:

$$
f(X) = \sum^M_{m=1} c_m * 1_{(X \in R_m)}
$$

Linear model works well if if features and responses are well approximated and have an underlying linear relationship. If there is a non-linear or complex relationship within the model then the decision tree might outperform the linear model. 



# --> Stopped at 307