---
title: "007_TreeBasedMethods"
output: html_document
---

# Tree Based Methods

We basically use stratifying or segmenting predictors space into simple regions, typically using the mean or the mode. Such methods are called decision tree methods. 

## Regression Trees

Are used to predict quantitative responses.

### Prediction via Stratification of the Feature Space

The process for regression trees is as the following:

1. We divide the predictor space into non-overlapping regions

2. For every observation we that falls into one region, we make the same prediction. Simply the mean of the response values.

To create the regions we simply create high-dimensional rectangles. (boxes) We seperate these boxes by minimizing the RSS. As it is impossible to find every box, we simply use recursive binary splitting. 

### Tree Pruning

The above process is likely to overfit the data on the test set, as the resulting fit might be too complex. A tree that is smaller leads to better variance, with a small bias. 

We can obtain an "optimal" tree by growing a very large tree and then *prune* it back in order to obtain a *subtree*. 

How do we determine the best way to prune the tree? Basically we want a subtree with the lowest test error rate. 

Weakest link pruning (Cost complexity pruning) is a viable way to do this. 

## Classification Trees

Classification Trees are used to predict a qualitative response rather than a quantitative one. With a classification tree we predict that each response belongs to the most commonly occurring class. 

Either the Gini index or the cross-entropy can be used for tree growing. However also the classification error rate can be used if global accuracy is needed. 

## Tree versus Linear Models

Linear regression assums the following model:

$$
f(X) = \beta_0 + \sum^p_{j=1} X_j \beta_j
$$

Tree based models assume the following form:

$$
f(X) = \sum^M_{m=1} c_m * 1_{(X \in R_m)}
$$

Linear model works well if if features and responses are well approximated and have an underlying linear relationship. If there is a non-linear or complex relationship within the model then the decision tree might outperform the linear model. 

## Ad-/Disadvantages of Trees

* Trees are easy to explain to people
* Trees might mirror more closely human decision making
* Trees can easily be shown graphically
* Trees do not have the same predictive accuracy as other methods

By using bagging, boosting and random forest the prediction accuracy can be substantially improved.

# Bagging, Random Forest and Boosting

## Bagging

Bootstrapping, bagging or aggregation is a general procedure to reduce the variance of a statistical learning method. 

Averaging leads to low variance. Hence if we take a number of training sets and average them in order to obtain a single statistical model.

As we do not usually have a number of training sets available we bootstrap and obtain predictions from each draw, then we average the predictions that we get. This method is called bagging. 

In order to make bagging useful for classification we simply record the class predicted by each of the $B$ trees and take a majority vote for the overall prediction. 

## Out of Error Estimation (OOB)

The key to bagging is to bagging is that trees are fit repeatedly to bootstrapped subsets of observations. It can be shown that on average each bagged three make 2/3 of the observations. The remaining 1/3 are called out of bag observations. 

The resulting test OOB error can then be commuted similar to MSE. 








# --> Stopped at 323