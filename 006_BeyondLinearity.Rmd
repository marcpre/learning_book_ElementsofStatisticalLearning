---
title: "006_BeyondLinearity"
output: html_document
---

#Moving Beyond Linearity

Besides staying with the linear model, we can lossen the assumption of linearity and examine simple extensions such as:

* *Polynomial Regression:* Extend the linear model by adding extra predictors that are raised to the power
* *Step functions:* These functions cut the range of K variables into several regions in order to fill piecewise a constant function.
* *Regression splines:* Basically, they divide the range of X into K distinct regions and fit a polynomial function to the data. However, these polynomial functions are constraint and fit the boundaries. This can in fact produce an extremely flexible fit.
* *Smoothing splines:* We basically minimize the residual sum of squares criterion subject to a smoothness penalty.
* *Local regression:* Similar to splines, BUT the regressions are allowed to overlap.
* *Generalized additive models:* Extend the above methods to deal with multiple predictors.

## Polynomial Regression

We fit the model using a polynomial function:

$$
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3 + ... + \beta_dx_i^d + \epsilon_i
$$

where $\epsilon_i$ is the error term.

Generally speaking we use a 3rd or 4th degree polynomial.

What is the Variance for a polynomial regression? Basically we get the variance for each coefficient. The estimated pointwise standart error of this variance is the square root of this variance. 

## Step Functions

Using a polynomial function imposes a global structure on the non-linear function of X. Instead we can also use step functions. Basically we break a constant into bins and fit a different category on each bin.

Hence, we can create cutpoints $c_1,c_2, ..., c_K$ on $X$ and then we construct $K+1$ new variables.

Picewise constant functions are best used when there are natural breakspoints in the predictors

## Basis Functions

Polynomial and Step functions are special cases of a basis function approach. The basic idea is that the function is is known in advance. 

## Regression Splines

### Piecewise Polynomials

Basically we fit a polynomial regression model to a cubic form:

$$
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3 + \epsilon_i
$$

If the polynomial has a single knot it takes the form and has a constraint with $x_i < c$.

### Constraints and Splines

In general cubic splines with $K$ knots use $4 + K$ degrees of freedom.







# --> Stopped at 277