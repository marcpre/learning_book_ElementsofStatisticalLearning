---
title: "Basics of Statistical Learning"
output: html_notebook
---

# What is statistical learning?

For given input varibales - can also be named predictors, independent variables, features - output variables - also named response or dependant variable and typically denoted using the symbol $Y$.

$$Y = f(X) + \epsilon$$
$f$ is the unknown function of $X_1, ..., X_n$ and $\epsilon$ is the error term, which is independent of $X$ and has the mean $0$.

## Why estimate $f$

There are two main reasons: $inference$ and $prediction$:

### Prediction

In case we have a set of inputs, such as $X$ available, and want to estimate the output $Y$, $f$ is not that important to use as long is predicts the right $Y$.

However, the accuracy of the predictions depends on two quantities:

* Reducible Error --> This error can be reduced by using better statistical techniques
* Irreducible Error --> Is always there!

The irreducible error cannot be reduced als $Y = f(X) + \epsilon$, whereas $\epsilon$ may contain unmeasured variables that are useful for predicting $Y$.(That's why it is also never $0$!)


### Inference

Focuses on the way that $Y$ is affected as $X_1,...,X_p$ changes.


### How can $f$ be estimated?

There are two types, linear and non-linear approaches.

#### Parametric Methods

1. First we make an assumption about the function and form. F.ex.: If $f$ is linear.
2. Secondly, we try to estimate the coefficience - parameters - f.ex.: using ordinary least squared

#### Non-Parametric Methods
Do not make an assumption about $f$ beforehand, but try to estimate $f$ as close to the data points as possible. 

### Trade-Off between Accuracy and Model Interpretability
Some models are less flexible, that means they can only produce a relatively small range of shapes for $f$.

#### Why would we ever chooose to use a more restrictive method instead?
F.ex.: if we are mainly interested in inference then these models are better as they aer more interpretable. 

However, it is often the case that linear models, that seem to be less accurate, provide a higher degree of accuracy to obtain forecasts.

## Supervised vs. Unsupervised Learning

### Supervised

For each observation such as $x_i, i=1,...,n$ there is an associated responds $y_i$. We basically learn the "mapping" function from the input to the output.

The clear goal is to estimate the mapping function so well, that we can predict the output if we only have inputs.

### Unsupervised

Unsupervised learning is if we only have input variables, but no output variables accordingly. 

One goal of unsupervised learning is to model the underlying distribution of the data to learn more about the input variables.

The difference between unsupervised and supervised learning is that, within supervised learning there is a "teacher" and within unsupervised learning not. Hence, there is no correct answer in unsupervised learning.

### Semi-Supervised 

We have only some input with corresponding output varibales. One reason might be that some output variables are too "expensive" to collect. 

## Regerssion vs. Classification

Variables van either be quanitative or qualitative(also known as categorical). Quantitative variables take a numerical value, qualitative variables take on a value in one of K different classes, or categories. 

*Regression problems* are using onla real - quantitative - values, while those involving qualitative values are referred to as *classification problems*. 

# Accuracy

## MSE (Mean Squared Error)

For regression problems we can use the MSE(mean squared error).

### Interpretation

An MSE of zero, meaning that the estimator predicts observations of the parameter $\theta$  with perfect accuracy. This is the ideal, but is typically not possible.


## The Bias-Variance Trade-Off

The expected test MSE for a given $x_0$, can always be decomposed into the sum of three fundamental quantities: the variance of $f(x_0)$, the square bias of $f(x_0)$ and the variance of the error term $\epsilon$.

Whenever accuracy of a model is discussed, it's important to understand prediction errors (bias and variance). However, there is a trade-off between minimizing a model's bias and variance. 

### What is a bias?

Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. The model is basically underfittet.

![Bias-Variance Trade-Off](https://cdn-images-1.medium.com/max/1600/1*9hPX9pAO3jqLrzt0IE3JzA.png)

## Classification Setting

The most common approach for classifying $f$ is the training **error rate**, the proportion of mistakes that we apply our estimate $f$ for the training observation.

A good classifier is one for which the test error is smallest. This can be generalized into the Bayes Classifier.

An algorithm that applies Bayes Classifier is KNN. KNN works the following way. It looks in its neighbourhood for data points and classifies them properly. If there is a larger conditional probability that one or the other classifier is stronger, the "neighbourhood" is classified accordingly.




