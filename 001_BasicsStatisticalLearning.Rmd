---
title: "Basics of Statistical Learning"
output: html_notebook
---

# What is statistical learning?

For given input varibales - can also be named predictors, independent variables, features - output variables - also named response or dependant variable and typically denoted using the symbol $Y$.

$$Y = f(X) + \epsilon$$
$f$ is the unknown function of $X_1, ..., X_n$ and $\epsilon$ is the error term, which is independent of $X$ and has the mean $0$.

## Why estimate $f$

There are two main reasons: $inference$ and $prediction$:

### Prediction

In case we have a set of inputs, such as $X$ available, and want to estimate the output $Y$, $f$ is not that important to use as long is predicts the right $Y$.

However, the accuracy of the predictions depends on two quantities:

* Reducible Error --> This error can be reduced by using better statistical techniques
* Irreducible Error --> Is always there!

The irreducible error cannot be reduced als $Y = f(X) + \epsilon$, whereas $\epsilon$ may contain unmeasured variables that are useful for predicting $Y$.(That's why it is also never $0$!)


### Inference

Focuses on the way that $Y$ is affected as $X_1,...,X_p$ changes.


### How can $f$ be estimated?

There are two types, linear and non-linear approaches.

#### Parametric Methods

1. First we make an assumption about the function and form. F.ex.: If $f$ is linear.
2. Secondly, we try to estimate the coefficience - parameters - f.ex.: using ordinary least squared

#### Non-Parametric Methods
Do not make an assumption about $f$ beforehand, but try to estimate $f$ as close to the data points as possible. 

### Trade-Off between Accuracy and Model Interpretability
Some models are less flexible, that means they can only produce a relatively small range of shapes for $f$.

#### Why would we ever chooose to use a more restrictive method instead?
F.ex.: if we are mainly interested in inference then these models are better as they aer more interpretable. 

However, it is often the case that linear models, that seem to be less accurate, provide a higher degree of accuracy to obtain forecasts.

## Supervised vs. Unsupervised Learning

### Supervised

### Unsupervised

---> Stopped at 26

```{r}
# plot(cars)
```
