---
title: "008_SupportVectorMachines"
output: html_document
---

# Support Vector Machines

SVMs were developed in the 1990 and are often considered as the best "out-of-the-box" classifiers.

## Maximal Margin Classifier

### What is a hyperplane?

In a p-dimensional setting a hyperplane can be defined as:

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0 
$$

In a two-dimensional setting a hyperplane is simply a line. 

We can detemine if the above equation does not hold true, then we simply say:

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p > 0
$$

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p < 0
$$

That means that the point lies simply on one or the other side of the line. 

### Classification Using a Separating Hyperplane

We can use separating hyperplane to create a classifier to separate two classes. If a separating hyperplane exist we can use it to construct a natural classifier. Basically we asign each observation a class such as ${-1,1}$. 

A class that is based on a separating hyperplane leads to a linear decision boundary.

## Maximal Margin Classifier

In general if our data can be sepearated using a hyperplane, we can construct an infinite number of such hyperplanes.

We must choose using a reasonable way, which hyperplane to choose. A natural choice is the maximal margine hyperplane, which is the separating hyperplane that is farthest from the training observations. 
We basically compute the perpendicular distance from each training observation to a give hyperplane. The smallest distance is called: margin. As the next step we can then classify a test observation based on which side of the maximal margin classifier it lies. 

The classifier can lead to overfitting if $p$ is large.

## Construction of the Maximal Margin Classifier

Is basically used in ML to give an associated distance from the decision boundary for each observation. 

## The Non-Separable Case

What if no separating hyperplane exist. We can basically extend the hyperplane concept and use a soft margin. If we generalize the maximum margine classifier to the non-separable case it is calle support vector classifier. 

# Support Vector Classifier

## Overview of the Support Vector Classifier

Sometimes it is impossible to find a classifier that fully separates the raw classes. Basically we are also happy if we find something that:

* separates most of the training observatiosn
* which brings greater robustness into the data

Using the support vector classifier, we allow some observatiosn to be on the "wrong side" of the margin. 

## Support Vector Classifier

The Support Vector Classifier, classifies observations depending on which side of a hyperplane it lies. The hyperplane is used to put each observations into the "right" class. However some observations may be misclassified. The parameter $C$ basically uses the margin to fit the hyperplanes between the observations. 





# --> Stopped at 349