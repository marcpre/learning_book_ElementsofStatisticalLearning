---
title: "008_SupportVectorMachines"
output: html_document
---

# Support Vector Machines

SVMs were developed in the 1990 and are often considered as the best "out-of-the-box" classifiers.

## Maximal Margin Classifier

### What is a hyperplane?

In a p-dimensional setting a hyperplane can be defined as:

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0 
$$

In a two-dimensional setting a hyperplane is simply a line. 

We can detemine if the above equation does not hold true, then we simply say:

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p > 0
$$

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p < 0
$$

That means that the point lies simply on one or the other side of the line. 

### Classification Using a Separating Hyperplane

We can use separating hyperplane to create a classifier to separate two classes. If a separating hyperplane exist we can use it to construct a natural classifier. Basically we asign each observation a class such as ${-1,1}$. 

A class that is based on a separating hyperplane leads to a linear decision boundary.

## Maximal Margin Classifier

In general if our data can be sepearated using a hyperplane, we can construct an infinite number of such hyperplanes.

We must choose using a reasonable way, which hyperplane to choose. A natural choice is the maximal margine hyperplane, which is the separating hyperplane that is farthest from the training observations. 
We basically compute the perpendicular distance from each training observation to a give hyperplane. The smallest distance is called: margin. As the next step we can then classify a test observation based on which side of the maximal margin classifier it lies. 

The classifier can lead to overfitting if $p$ is large.







# --> Stopped at 346