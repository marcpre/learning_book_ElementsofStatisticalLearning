---
title: "010_UnsupervisedLearning"
output: html_document
---

# Unsupervised Learning

With unsupervised learning we only have a set of features $X_1, X_2, ... ,X_p$ measured on $n$ observations. The goal is to discover interesting things about the measurements. 

## The Challenge of Unsupervised Learning

Unsupervised Learning is often performed as part of an exploratory data analysis. However, it is hard how good our data is responding to the model. 

## Principal Component Analysis (PCA)

When having a large dataset, PCA allows us  to summarize the variables into smaller, that explain most of the variability in the original set. PCA refers to the process of where principal components are computed and the subsequent use of these components in understanding the data. 

### What are principle components

Suppose that we want to visualize $n$ observations with measurements on a set of $p$ features. We could use a scatterplot, however there are $p \choose 2$ such scatterplots. Hence, a better approach is to only get for $n$ observations where $p$ is the largest. 

PCA provides a tool to find a low-dimensional representation of the data set, where as much information is captured. 

The first principal component is the normalized linear combination of the features that has the largest variance. 

Geometrically we project the original data down onto the subspace spanned by the principal components and plotting the projected points. 
 
## Another interpretation of PCA

Basically we want to find a dimension that lies as close as possible to all of the data points. The reason is that such a dimension will provide a good summary of the data.
 
## More on PCA

### Scaling the variables
 
When performing PCA, the variables should be centered by mean $0$. Typically we scale each variable with a Standartdeviation of $1$ before performing PCA. 

### Uniqueness of the Principal Components

Eah principal component vector is unique up to a sign flip. 

### The Proportion of Variance Explained

After performing PCA, we can ask the question: How much information in a given data set is lost by projecting the observations onto the first few principal components? Basically, we are interested in knowing the proportion of variance explained (PVE). 

### Deciding how many principal components to use

How many principal components are needed? We typically decide on the number of principal components by examining a scree plot. If in the scree plot the proportion of variance explianed drops of, we refer to this as elbow. 

However this question is ill defined as there is no right answer how many prinicpal components are enough. If no clear pattern is found in the first view principal components it the decision of the data scientist to use more.

### Other Uses for Principal Components

Many statistical techniques can use principal components as based features to create a model on top of it.


# Clustering Methods

Clustering refers to techniques to find subgroups or clusters in a data set. We basically want to join similar groups together and leave different groups out. Teh difference between PCA and clustering is the following:

* PCA tries to find low-dinemsional representations that explain a good fraction of the variance

* Clustering tries to find subgroups


## K-Means Clustering

K-Means Clustering is a simple and elegant approach for partitioning a dataset into $K$ distinct, non-overlapping clusters. To perform the algorithm we basically specify the desired number of clusters. 

The idea of K-means clustering is that the within-cluster variation is as small as possible. We basically want to minimize the amount by which the observations within a cluster differ from each other. We basically can use the common euclidian distance. 

The algorithm runs as following:

1. Assign anumber - from 1 to K - to each of the observations.

2. Iterate:

2.1 For each of the $K$ clusters, compute the centroiod. Centroid: is the vector of the $p$ feature means for the observation in the kth cluster

2.2 Asign each observation to the cluster whose centroid is closest.





# --> Stopped at 390