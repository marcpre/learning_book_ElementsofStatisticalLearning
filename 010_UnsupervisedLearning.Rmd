---
title: "010_UnsupervisedLearning"
output: html_document
---

# Unsupervised Learning

With unsupervised learning we only have a set of features $X_1, X_2, ... ,X_p$ measured on $n$ observations. The goal is to discover interesting things about the measurements. 

## The Challenge of Unsupervised Learning

Unsupervised Learning is often performed as part of an exploratory data analysis. However, it is hard how good our data is responding to the model. 

## Principal Component Analysis (PCA)

When having a large dataset, PCA allows us  to summarize the variables into smaller, that explain most of the variability in the original set. PCA refers to the process of where principal components are computed and the subsequent use of these components in understanding the data. 

### What are principle components

Suppose that we want to visualize $n$ observations with measurements on a set of $p$ features. We could use a scatterplot, however there are $p \choose 2$ such scatterplots. Hence, a better approach is to only get for $n$ observations where $p$ is the largest. 

PCA provides a tool to find a low-dimensional representation of the data set, where as much information is captured. 

The first principal component is the normalized linear combination of the features that has the largest variance. 

Geometrically we project the original data down onto the subspace spanned by the principal components and plotting the projected points. 
 
## Another interpretation of PCA

Basically we want to find a dimension that lies as close as possible to all of the data points. The reason is that such a dimension will provide a good summary of the data.
 
## More on PCA

### Scaling the variables
 
When performing PCA, the variables should be centered by mean $0$. Typically we scale each variable with a Standartdeviation of $1$ before performing PCA. 

### Uniqueness of the Principal Components

Eah principal component vector is unique up to a sign flip. 

### The Proportion of Variance Explained

After performing PCA, we can ask the question: How much information in a given data set is lost by projecting the observations onto the first few principal components? Basically, we are interested in knowing the proportion of variance explained (PVE). 




# --> Stopped at 387