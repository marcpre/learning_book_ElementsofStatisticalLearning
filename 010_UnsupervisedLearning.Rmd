---
title: "010_UnsupervisedLearning"
output: html_document
---

# Unsupervised Learning

With unsupervised learning we only have a set of features $X_1, X_2, ... ,X_p$ measured on $n$ observations. The goal is to discover interesting things about the measurements. 

## The Challenge of Unsupervised Learning

Unsupervised Learning is often performed as part of an exploratory data analysis. However, it is hard how good our data is responding to the model. 

## Principal Component Analysis (PCA)

When having a large dataset, PCA allows us  to summarize the variables into smaller, that explain most of the variability in the original set. PCA refers to the process of where principal components are computed and the subsequent use of these components in understanding the data. 

### What are principle components

Suppose that we want to visualize $n$ observations with measurements on a set of $p$ features. We could use a scatterplot, however there are $p \choose 2$ such scatterplots. Hence, a better approach is to only get for $n$ observations where $p$ is the largest. 

PCA provides a tool to find a low-dimensional representation of the data set, where as much information is captured. 

The first principal component is the normalized linear combination of the features that has the largest variance. 

Geometrically we project the original data down onto the subspace spanned by the principal components and plotting the projected points. 
 
## Another interpretation of PCA

Basically we want to find a dimension that lies as close as possible to all of the data points. The reason is that such a dimension will provide a good summary of the data.
 
## More on PCA

### Scaling the variables
 
When performing PCA, the variables should be centered by mean $0$. Typically we scale each variable with a Standartdeviation of $1$ before performing PCA. 

### Uniqueness of the Principal Components

Eah principal component vector is unique up to a sign flip. 

### The Proportion of Variance Explained

After performing PCA, we can ask the question: How much information in a given data set is lost by projecting the observations onto the first few principal components? Basically, we are interested in knowing the proportion of variance explained (PVE). 

### Deciding how many principal components to use

How many principal components are needed? We typically decide on the number of principal components by examining a scree plot. If in the scree plot the proportion of variance explianed drops of, we refer to this as elbow. 

However this question is ill defined as there is no right answer how many prinicpal components are enough. If no clear pattern is found in the first view principal components it the decision of the data scientist to use more.

### Other Uses for Principal Components

Many statistical techniques can use principal components as based features to create a model on top of it.


# Clustering Methods

Clustering refers to techniques to find subgroups or clusters in a data set. We basically want to join similar groups together and leave different groups out. Teh difference between PCA and clustering is the following:

* PCA tries to find low-dinemsional representations that explain a good fraction of the variance

* Clustering tries to find subgroups


## K-Means Clustering

K-Means Clustering is a simple and elegant approach for partitioning a dataset into $K$ distinct, non-overlapping clusters. To perform the algorithm we basically specify the desired number of clusters. 

The idea of K-means clustering is that the within-cluster variation is as small as possible. We basically want to minimize the amount by which the observations within a cluster differ from each other. We basically can use the common euclidian distance. 

The algorithm runs as following:

1. Assign anumber - from 1 to K - to each of the observations.

2. Iterate:

  2.1 For each of the $K$ clusters, compute the centroiod. Centroid: is the vector of the $p$ feature means for the observation in the kth cluster

  2.2 Asign each observation to the cluster whose centroid is closest.


## Hierachical Clustering

One potential disadvantage of K-Means Clustering is that it requires us to pre-specify the number of clusters $K$.

### Interpretation of a Dendrogram

Observations that fuse together at the bottom of the tree are quite similar and observations that fuse together at the top are will tend to be quite different. Conclusions about the similarity of an observation can be made when looking at the vertical axis of a dendrogram. 

### The Hierarchical Clustering Algorithm

We define some sort of dissimilarity measure - such as euclidian measure - between each pair of observations. 

Linkage defines dissimilarity between a pair of groups of observations. There are 4 common types: complete, average, single and centroid. Average and complete are generally preferred by statisticians. Centroid has a major drawback as inversion - two clusters are fused at a height below either of te individual clusters in the dendrogram. 

### Choice of dissimilarity measure

Usually euclidian distance is used as dissimilarity measure. However, we can also choose correlation-based dissimilarity: It is considered similar if two observations are highly correlated. 

### Practical issues in clustering

#### Small Decisions with Big Consequences

Before we cluster the following decisions have to be made:

  1. Should the observations be standartized?
  2. In the case of hierachical clustering:
  2.1 What dissimilarity measure should be used?
  2.2 What type of linkage should be used?
  2.3 Where should the dendrogram be cut, in order to obtain the clusters?
  3. In case of K-Means, how many clusters should we look for?

#### Validating the clusters obtained

When we do clustering, we will definitely find clusters. We can support this via p-values. There is no single **best** way for this yet. 

#### Other considerations in clustering

Outliers are handled quite bad by K-Means. Therefore, mixure models are a good way to handle this. There is also a version of soft K-Means clustering. 

#### Interpreting Clustering results

* Play with the parameters to see which patterns arise from the data
* Cluster only a subset to get a sense of the robustness






# --> Stopped at 440