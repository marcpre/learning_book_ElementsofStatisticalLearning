---
title: "005_LinearModelSelectionAndRegularization"
output: html_document
---

# Linear Model Selection and Regularization

The linear model can be extended by using another fitting procedure, instead of least squares. Why is is positiv to use another fitting procedure?

* Prediction Accuracy. If n > p - number of observations is larger than number of variables - then the least squares estimates tend to also have low variance. If n is smaller than p then there can be a lot of variability in the least squares fit, resulting in overfitting. 

* Model Interpretability:  Often many variables in a linear regression are not associated with the response. Setting the coefficients to 0 we can determine which features are really needed. 

## Subset

### Best Subset Selection

We fit a seperate least square regression for each possible combination of the p predictors. After that we try to identify all the ones that fit best. 

The algorithm works as the following:

1. Select the null model - $M_0$ - for each predictor. This model simply predicts the sample mean for each observation.

2. For k = 1,2, ... ,p
2a. Fit all models that contain exactly k predictors
2b. Pick the best model amonge these ${p\choose k}$

3. Select a single best model using cross-validated prediction error, $C_p (AIC), BIC$ or adjusted $R^2$.

The main disadvantages of this approach is that in generall there have to be $2^n$ considered.

### Stepwise Selection

For large $p$ it is computationally hard to select the rigth variables. Stepwise regression provides an alternative to explore far more restrictive models.

## Forward Stepwise Selection
Forward Stepwise Selection is a good alternative to best subset selection. 

This is how it works? Forward stepwise selection ads one predictor at a time to the model until all predictors are within the model. Only variables are added to the model, which give at the step time the greatest additional improvement to the model. For forward regression mainly $1 + p(p+1)/2$ model are fit. So basically if $p=20$ it requires fitting only $211$ models. 

For choosing the right model we can either use the RSS or highest $R^2$. 

There is not guarantee if you use Forward Stepwise Regression to find the best model. For example if the best 1-variable model is $X_1$ and the best 2-variable model is $X_2$ and $X_3$, then forward stepwise regression will fail. The reason is that it will fail to identify the best possible model, as $M_1$ will contain $X_1$ with one additional variable. 

## Backward Stepwise Selection
Similar to forward stepwise regression we push variables out. However, the main difference is that we do not add variables but remove variables from the dataset. The least useful predictor is removed, one-at-a-time. 

One requirement of backward stepwise selection is that the number of samples $n$ is larger than the number of variables $p$

## Hybrid Approaches
The best approaches give similar but not identical models. There are a number of further approaches that tend to focus more on basic subset selection, but use forward/backward stepwise selection to bring down the computational advantage.

## Choosing the optimal model

Using smallest RSS, largest R^2 are good estimators for checking if the model has a good fit. However, the test error is not a good fit for estimating the optimal model. Hence, we have to identify the correct training error. 

## $C_p$, AIC, BIC, and Adjusted $R^2$

The training set MSE is usually an underestimate of the test MSE. The reason for this is that if we try to estimate a regression coefficient, we usually try to minimize the training data using least squares, we specifically estimate the regression coefficients such as the training RSS. Hence, the test error may not decrease. However, we can **adjust** the training error for the model size. There are 4 ways for adjusting:
$C_p$, AIC, BIC, and Adjusted $R^2$

We can use one of the above approaches to select the best model.

## Validation and Cross-Validation
We can  calculate the test-error and pick the best model based on this. 

However, if we repeatetly change the split of the data and the validation set, the test error might change. So we can use the *one-standard-error rule*. Basically, we first calculate the standard error of the estimated test MSE for each model size, and then select the model that has the lowest test error that is within 1 standard error of the lowest on the curve. 

# Shrinkage Methods

As an alternative we can use shrinkage methods to constrain or regularize the coefficient estimates towards 0. There are two methods for shrinkage *ridge regression* and *lasso*.

## Ridge Regression

$$
RSS + \lambda \sum^p_{j=1} \beta^2_j
$$

* $ \lambda \geq 0$ is a tuning parameter
* $ \sum^p_{j=1} \beta^2_j $ is called shrinkage penalty

### Why does ridge regression improve over least squares?
Basically, the answer lies in the bias-variance trade-off. As $\lambda$ increases the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. Hence, ridge regression works best in the situations when least squared estimates have a high variance.

## The Lasso

Ridge Regression does have one obvious disadvantage. The method includes all $p$ predictors in the final model. The shrinkage $\lambda$ will shrink all coefficients towards $0$. This is not a problem for model accuracy, but for interpretability of the model. The difference between ridge and lasso regression is the use of the $|\beta_j|$ term in the ridge regression penalty.  In case of the lasso regression the tuning parameter $\lambda$ forces some parameters towards zero, when it is sufficiently large.

$$
RSS + \lambda \sum^p_{j=1} |\beta_j|
$$

### Comparing Lasso and Ridge Regression

Lasso has one main advantage over ridge regression, it produces simpler and more interpretable modelsthat involve only a subset of the predictors.

However, lasso tends to outperform ridge regression in terms of bias, variance and MSE.

## Select a tuning parameter

Lasso and Ridge Regression require to select $\lambda$, the tuning parameter. We can simply compute a grid of parameters and pick for which the cross-validation error is smallest. 



# Dimension Reduction Methods

The above methods have controlled variance in two different ways, either by selecting a subset of of the original variables or by shrinking their coefficients towards zero. With Dimension Reduction Methods we basically transform the predictor variables and then fit a least squared model using the newly transformed variables.


Basically we choose a constant $\phi$ and transform the variables. Then we perform regression on top of it. We can choose the constant $\phi$ either by principal component analysis or partial least squares.

## Principle Component Analysis (PCA)

PCA is a typical method to derive a low-dimensional set of features from a large set of variables. We can think of PCA, that it creates a single number that summarieses a component.

Basically we would like to reduce the dataset by showing string patterns. 








# --> Stopped at 236