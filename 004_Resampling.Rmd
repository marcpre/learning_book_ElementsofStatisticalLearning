---
title: "004_Resampling"
output: html_document
---

# Resampling

By Resampling methods we understand repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. 

Two methods are sound: cross-validation and bootstrapping. Cross-Validation can be used to estimate the test error associated with a given statistical method. Bootstrapping is used in several settings, mainly to provide a measure of accuracy of a parameter estimate or a given statistical learning method.

## Cross-Validation

The test error is the average error that comes up when trying to predict a response on a new observation that was NOT used in a training sample.

When no test set is available holding out data can be used.

### The Validation Set Approach
Why do we do it?
Validation is done to estimate the test error rate in a dataset.

Basically we randomly divide the available set into two parts, a training set and a validation set or hold out set. The resulting validation set error rate provides an estimate of the test error rate.

The validation set approach is easy to implement, BUT has two drawbacks:

1. The validation estimate of the test error rate can be highly variable, depending on precisely on which observation are included in the training set and which observations are included in the test set.

2. Statistical models tend to perform worse on less observations the validation set error could be overestimated for the model fit on the entire data set.

### Leave-One-Out Cross-Validation (LOOC)
Basically the set is split into two parts. However, we do not create two subsets of comparable size, but instead leave a single observation out that we try to predict. The model is then fit on the $n-1$ training observations. Using $MSE = (y_1 - \hat{y_1})^2$ provides an unbiased estimate for the test error.

The LOOC is the average of all $n$ test MSE's.

The advantages of this approach are:
* Less biased
* LOOC will always give the same result as there is no randomness in the data

The disadvantages of this approach are:
* Extremely computing expensive

### K-Fold Cross-Validation
This method basically involves dividing the set into k groups - also called folds - of approximately equal size. The first fold is the validation set and the rest of the folds  - $k-1$ - are using the method. Basically, we compute for each fold the $MSE_1, MSE_2,..., MSE_k$. The k-fold CV is then computed by taking the average of all $MSE$ values. 

### Bias-Variance Trade-Off for k-Fold Cross-Validation
k-fold CV has some advantages over LOOC in terms of accuracy and the produced error rate: * k-fold cross validation is less computing intesive
* Using $k=5$ or $k=10$ is an empirically good indicator for creating the folds

### Cross-Validation on Classification Problems

The LOOCV error rate takes the form:

$$
CV_{(n)}= \frac{1}{n}\sum_{i=1}^n Err_i
$$
where $Err_i = I(y_i \neq \hat{y}_i$



# ---> 189 stopped at