---
title: "002_LinearRegression"
output: html_document
---

# simple Linear Regression
Simple concept for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is a linear relationship between $X$ and $Y$.

$$
Y \approx \beta_0 + \beta_1X
$$

## Estimating the coefficients

By minimizing the residuals via the least square approach we can fit the line properly. 

### Assessing the Accuracy of the Coefficient Estimates

We can use the standart error to compute how far off the average of the model is away from the true value. The standart error can also be used to compute confidence intervals. 

#### p-value
The p-value is interpreted as the following: That it is unlikely to observe a substantial association between the predictor and the response. Hence, a small p-value says that we can infer that there is a relationship between the predictor and the response.


## Model Accuracy

By measuring accuracy we want to find out how much the model fits the underlying data.

### Residual Standard Error

The RSE is considered a measure of the lack of fit of the model to the data. If the RSE is large it indicates that the model might NOT fit to the underlying data.

### R^2 Statistic

R^2 describes a proportion - the proportion of variance explained. It is a value between 0 and 1. R^2 has an interpretation advantage over RSE. 

  1. Its value is between 0 and 1
  2. It shows the strength of the linear relationship in the data. It is a measure of the linear relationship between X and Y.


# Multiple Linear Regression

Simply said a multiple linear regression can be represented as trying multiple linear regressions against 1 predictor. 

Multiple Linear regression can be represented as,

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p +  \epsilon
$$

## Estimate Regression Coefficients

We can use similarly to the linear regression the "least squared"-method, to minimize the sum of of squared results.


## Important Questions

  1. Is at least one predictor useful in predicting a response?
  2. Is only a subset of predictors useful or are all predictors useful?
  3. How well does the model fit the data?
  4. Given a set of response values, what response should we predict and how accurate is our prediction?

### 1. Test the $H_0$-Hypotheses

The first step is to use a F-Statistic and compute a p-value to test the significane of the $H_0$-Hypotheses. 
  
### 2. Decide on the most important variables

Using variable selection, we can see which variable has the highest impact on the model. Ideally we would like to try any number of combination to select a variable. 

* *Forward Selection:* We start with the null model - the model that contains an intercept, but no predictors. As a next step we fit p simple linear regression and add to the null model the variables that result in the lowest RSS. 
* *Backward Selection:* We start with all variables in the model and remove the variable with the greatest p-value. This procedure is continued and we stop it when we approach a certain threshold.
* *Mixed Selection:* This is a mix of the two models above. We start with forward selection and then take gradually variables away. We stop when we are at a certain threshold.
  
### 3. Model Fit
Using $R^2$ and $RSE$ we can detemine the model fit. Some properties are:

1. $R^2$ equals the $Cor(Y, \hat{Y})$




#### ---> Stopped at 79