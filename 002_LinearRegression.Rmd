---
title: "002_LinearRegression"
output: html_document
---

# simple Linear Regression
Simple concept for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is a linear relationship between $X$ and $Y$.

$$
Y \approx \beta_0 + \beta_1X
$$

## Estimating the coefficients

By minimizing the residuals via the least square approach we can fit the line properly. 

### Assessing the Accuracy of the Coefficient Estimates

We can use the standart error to compute how far off the average of the model is away from the true value. The standart error can also be used to compute confidence intervals. 

#### p-value
The p-value is interpreted as the following: That it is unlikely to observe a substantial association between the predictor and the response. Hence, a small p-value says that we can infer that there is a relationship between the predictor and the response.


## Model Accuracy

By measuring accuracy we want to find out how much the model fits the underlying data.

### Residual Standard Error

The RSE is considered a measure of the lack of fit of the model to the data. If the RSE is large it indicates that the model might NOT fit to the underlying data.

### R^2 Statistic

R^2 describes a proportion - the proportion of variance explained. It is a value between 0 and 1. R^2 has an interpretation advantage over RSE. 

  1. Its value is between 0 and 1
  2. It shows the strength of the linear relationship in the data. It is a measure of the linear relationship between X and Y.


# Multiple Linear Regression

Simply said a multiple linear regression can be represented as trying multiple linear regressions against 1 predictor. 

Multiple Linear regression can be represented as,

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p +  \epsilon
$$

## Estimate Regression Coefficients

We can use similarly to the linear regression the "least squared"-method, to minimize the sum of of squared results.


## Important Questions

  1. Is at least one predictor useful in predicting a response?
  2. Is only a subset of predictors useful or are all predictors useful?
  3. How well does the model fit the data?
  4. Given a set of response values, what response should we predict and how accurate is our prediction?

### 1. Test the $H_0$-Hypotheses

The first step is to use a F-Statistic and compute a p-value to test the significane of the $H_0$-Hypotheses. 
  
### 2. Decide on the most important variables

Using variable selection, we can see which variable has the highest impact on the model. Ideally we would like to try any number of combination to select a variable. 

* *Forward Selection:* We start with the null model - the model that contains an intercept, but no predictors. As a next step we fit p simple linear regression and add to the null model the variables that result in the lowest RSS. 
* *Backward Selection:* We start with all variables in the model and remove the variable with the greatest p-value. This procedure is continued and we stop it when we approach a certain threshold.
* *Mixed Selection:* This is a mix of the two models above. We start with forward selection and then take gradually variables away. We stop when we are at a certain threshold.
  
### 3. Model Fit
Using $R^2$ and $RSE$ we can detemine the model fit. Some properties are:

1. $R^2$ equals the $Cor(Y, \hat{Y})$
2. $R^2$ close to 1 indicates that the model explains a large portion of the variance in the response variable.
3. $R^2$ usually decreases if more predictors are added to the model

### 4. Predictions
 
Making predictions in the model, we use simply the estimated coefficients. However, there are three uncertainties associated with the prediction:

1. Reducible error - We can compute confidence intervals to see how "confident" we can be about the coefficients
2. In practice fitting a linear model is always trying to fit reality. Hence, there is also the model bias.
3. Irreducible error - Even if we knew the true value, we cannot be absolutely certain, because there is still the irreducible error - $\epsilon$.

## Other considerations

### Qualitative Predictors

Besides quantitive variables there can also be qualitative variables such as, gender, student status, ethnicity.

#### Predictors with only two levels

If a qualitative predictor - also known as factor - has only two levels we can create a dummy variable which is either 1 or 0.

#### Predictors with more than two levels

If a single dummy variable has more than two levels then we can create several dummy variables to cope with this problem. So for example ethnicity then has for each factor several dummy variables.

## Extension of the Linear Model

Linear Regression makes two assumptions, so that the relationship between predictors and responses are:

*additive: That means that the changes in a predictor $X_j$ is independent on the responses $Y$ 
*linear: Means that the changes in the response $Y$ due to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$

### Non-Linear Relationship
Using polynomial regression, we can extend the linear regression model by non-linear releationships





#### ---> Stopped at 101