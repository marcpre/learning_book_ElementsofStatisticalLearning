---
title: "002_LinearRegression"
output: html_document
---

# simple Linear Regression
Simple concept for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is a linear relationship between $X$ and $Y$.

$$
Y \approx \beta_0 + \beta_1X
$$

## Estimating the coefficients

By minimizing the residuals via the least square approach we can fit the line properly. 

### Assessing the Accuracy of the Coefficient Estimates

We can use the standart error to compute how far off the average of the model is away from the true value. The standart error can also be used to compute confidence intervals. 

#### p-value
The p-value is interpreted as the following: That it is unlikely to observe a substantial association between the predictor and the response. Hence, a small p-value says that we can infer that there is a relationship between the predictor and the response.


## Model Accuracy

By measuring accuracy we want to find out how much the model fits the underlying data.

### Residual Standard Error

The RSE is considered a measure of the lack of fit of the model to the data. If the RSE is large it indicates that the model might NOT fit to the underlying data.

### R^2 Statistic

R^2 describes a proportion - the proportion of variance explained. It is a value between 0 and 1. R^2 has an interpretation advantage over RSE. 

  1. Its value is between 0 and 1
  2. It shows the strength of the linear relationship in the data. It is a measure of the linear relationship between X and Y.


# Multiple Linear Regression

Simply said a multiple linear regression can be represented as trying multiple linear regressions against 1 predictor. 

Multiple Linear regression can be represented as,

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p +  \epsilon
$$

## Estimate Regression Coefficients

We can use similarly to the linear regression the "least squared"-method, to minimize the sum of of squared results.


## Important Questions

  1. Is at least one predictor useful in predicting a response?
  2. Is only a subset of predictors useful or are all predictors useful?
  3. How well does the model fit the data?
  4. Given a set of response values, what response should we predict and how accurate is our prediction?

### 1. Test the $H_0$-Hypotheses

The first step is to use a F-Statistic and compute a p-value to test the significane of the $H_0$-Hypotheses. 
  
### 2. Decide on the most important variables

Using variable selection, we can see which variable has the highest impact on the model. Ideally we would like to try any number of combination to select a variable. 

* *Forward Selection:* We start with the null model - the model that contains an intercept, but no predictors. As a next step we fit p simple linear regression and add to the null model the variables that result in the lowest RSS. 
* *Backward Selection:* We start with all variables in the model and remove the variable with the greatest p-value. This procedure is continued and we stop it when we approach a certain threshold.
* *Mixed Selection:* This is a mix of the two models above. We start with forward selection and then take gradually variables away. We stop when we are at a certain threshold.
  
### 3. Model Fit
Using $R^2$ and $RSE$ we can detemine the model fit. Some properties are:

1. $R^2$ equals the $Cor(Y, \hat{Y})$
2. $R^2$ close to 1 indicates that the model explains a large portion of the variance in the response variable.
3. $R^2$ usually decreases if more predictors are added to the model

### 4. Predictions
 
Making predictions in the model, we use simply the estimated coefficients. However, there are three uncertainties associated with the prediction:

1. Reducible error - We can compute confidence intervals to see how "confident" we can be about the coefficients
2. In practice fitting a linear model is always trying to fit reality. Hence, there is also the model bias.
3. Irreducible error - Even if we knew the true value, we cannot be absolutely certain, because there is still the irreducible error - $\epsilon$.

## Other considerations

### Qualitative Predictors

Besides quantitive variables there can also be qualitative variables such as, gender, student status, ethnicity.

#### Predictors with only two levels

If a qualitative predictor - also known as factor - has only two levels we can create a dummy variable which is either 1 or 0.

#### Predictors with more than two levels

If a single dummy variable has more than two levels then we can create several dummy variables to cope with this problem. So for example ethnicity then has for each factor several dummy variables.

## Extension of the Linear Model

Linear Regression makes two assumptions, so that the relationship between predictors and responses are:

*additive: That means that the changes in a predictor $X_j$ is independent on the responses $Y$ 
*linear: Means that the changes in the response $Y$ due to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$

### Non-Linear Relationship
Using polynomial regression, we can extend the linear regression model by non-linear releationships

## Potential Problems

1. Non-linearity of the response-predictor relationship
2. Correlation of error terms
3. Non-constant variance of error terms
4. Outliers
5. High-leverage points
6. Collinearity


### 1. Non-linearity of the response predictor relationship

The linear model assums that there is a straight-line relationship between the predictors and the response. If the true relationship is not linear, OSL does not fit the model very good.

Residual plots are a perfect way to check non-linearity. A simple way to fix this problem is to use non-linear transformations of the predictors, such as log X, sqrt(X) and X^2.

### 2. Correlation of error terms

An important assumption of the linear model is that the error terms $\epsilon_1, \epsilon_2, ... ,\epsilon_n$ are uncorrelated. If the error terms are correlated, p-value can be lower and we risk to make wrong conclusions from the model. 

How can error terms be correlated? This phenomenon usually happens in time-series data. In order to determine if this is the case, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pattern, else tracking happens: That means, that the error terms are tracking each other, are relatively close.

### 3. Non-constant variance
Another important assumption of the linear model is that the $Var(e_i)=\sigma^2$. If there is no constant variance, we can point this out as heteroscedasticity.

When faced with such a problem the responses Y can be transformed using a concave function such as $log Y$ or $\sqrt(Y)$, which leads to a reduction in heteroscedasticity.

If we have a guess about the variance, we can created a weighted model.

### 4. Outlier
An outlier is a point for which $y_i$ is far from the value predicted by the model. The problem with outliers is that it can falsify the model. Using a residual plot, one can identify outliers. 
In practice it is not easy to identify how large a residual needs to be to consider it as an outlier. To address this problem, we can plot the studentized residuals, computed by dividing each residual $e_i$ by its estimated standard error.

### 5. High Leverage Points

High leverage observations are responses that have large impact on the predictors. In fact a high leverage point has an unusual large value for the reponse $x_i$. The problem with high-leverage points is that it can have a large impact on the predictors.

In order to quantify an observation's leverage, we can compute the leverage statistic. 

### 6. Collinearity
Collinearity refers to the situation where two or more predictor variables are closely related to each other.

The problem of collinearity can have several effects on a model as we cannot easily separate the predictors which have an effect on the response variables. 

If we have several variables that are correlating within the model we call this multi-collinearity.

We can detect (multi-)collinearity when computing the variance inflation factor (VIF). If the VIF is 1, this indicates the complete absence of collinearity. Typically, a VIF that exceeds 5 or 10 indicates a problematic amount of collinearity. 

When faced with collinearity there are two simple solutions:

* Drop the response variablethat are correlating
* Combine the correlated variables together in one single predictor




#### ---> Stopped at 103