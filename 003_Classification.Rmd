---
title: "003_Classification"
output: html_document
---

# Classification
In many cases the response variable is qualitative. Often to these variables we refer also that they are categorial. 

## Why NOT linear regression
Linear regression is not appropriate in the case of a qualitative response. If we have have several categories that do not align with each other then we could create a single model for each category. However, the dummy variable  approach can easily be used for variables that have two categorial values. 

## Logistic Regression
Should be used when the dependent variable is dichotomous (binary).

Logistic Regression is defined as the following:

![Definition Logistic Regression](https://www.statisticssolutions.com/wp-content/uploads/2010/01/log23.jpg)

The left side is called logit or log-odds.

### Estimate the Regression Coefficient
To estimate the coefficients $\beta_0$ and $\beta_1$, we can use the maximum likelihood method. Basically we try to optimize the $\beta$ using a likelihood function. $\beta_0$ and $\beta_1$ are choosen to maximize this likelihood function.

### Making Predictions
Once the coefficience have been estimated it 

## Multiple Logistic Regression

$$
log(p(X)/1-p(X)) =  \beta_0 + \beta_1X_1 + ... + \beta_pX_p
$$
where $X = (X_1,...,X_p$ and $p$ predictors. Hence,

$$
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}
$$

# Linear Discriminant Analysis

Logistic regression is very useful for direct modeling if we have only two response classes in a variable. In a statistical jargon we model the conditional distribution of the response Y. 

Linear Discrimination Analysis is useful if:

* Parameters are quite unstable if parameters are well separated. LDA does not suffer from this problem.
* If n is small and the distribution of the predictors $X$ is normal, then LDA is also more stable

## Using Bayes' Theorem for Classification

$$
Pr(Y = k|X = x) = \frac{\pi_kf_k(x)}{\sum\limits_{l=1}^K \pi_lf_l(x)}
$$

If we can approximate $f_k(x)$ then we can find a way to get the Baye's Classifier.


# >>> Stopped at 140