---
title: "003_Classification"
output: html_document
---

# Classification
In many cases the response variable is qualitative. Often to these variables we refer also that they are categorial. 

## Why NOT linear regression
Linear regression is not appropriate in the case of a qualitative response. If we have have several categories that do not align with each other then we could create a single model for each category. However, the dummy variable  approach can easily be used for variables that have two categorial values. 

## Logistic Regression
Should be used when the dependent variable is dichotomous (binary).

Logistic Regression is defined as the following:

![Definition Logistic Regression](https://www.statisticssolutions.com/wp-content/uploads/2010/01/log23.jpg)

The left side is called logit or log-odds.

### Estimate the Regression Coefficient
To estimate the coefficients $\beta_0$ and $\beta_1$, we can use the maximum likelihood method. Basically we try to optimize the $\beta$ using a likelihood function. $\beta_0$ and $\beta_1$ are choosen to maximize this likelihood function.

### Making Predictions
Once the coefficience have been estimated it 

## Multiple Logistic Regression

$$
log(p(X)/1-p(X)) =  \beta_0 + \beta_1X_1 + ... + \beta_pX_p
$$
where $X = (X_1,...,X_p$ and $p$ predictors. Hence,

$$
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}
$$

# Linear Discriminant Analysis

Logistic regression is very useful for direct modeling if we have only two response classes in a variable. In a statistical jargon we model the conditional distribution of the response Y. 

Linear Discrimination Analysis is useful if:

* Parameters are quite unstable if parameters are well separated. LDA does not suffer from this problem.
* If n is small and the distribution of the predictors $X$ is normal, then LDA is also more stable

## Using Bayes' Theorem for Classification

$$
Pr(Y = k|X = x) = \frac{\pi_kf_k(x)}{\sum\limits_{l=1}^K \pi_lf_l(x)}
$$

If we can approximate $f_k(x)$ then we can find a way to get the Baye's Classifier.

## Confusion Matrix
Is often used to describe a classification model. 

* *True positives (TP):* These are cases in which we predicted yes (they have the disease), and they do have the disease.
* *True negatives (TN):* We predicted no, and they don't have the disease.
* *False positives (FP):* We predicted yes, but they don't actually have the disease. (Also known as a "Type I error.")
* *False negatives (FN):* We predicted no, but they actually do have the disease. (Also known as a "Type II error.")

Further terms are often calculated out of the confusion matrix:

* *Accuracy:* How often is the classifier correct?
* *Precision:* When yes is predicted, how often is it correct?
* *Prevalence:* How often does the yes condition occur in our sample?

## ROC curves and Area Under the Curve 

ROC (Receiver Operating Characteristics) curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It basically, explains "how much the model is capable of distinguishing between classes". This is basically important as we can optimize on the True Positive rate. 

![ROC and AUC](https://cdn-images-1.medium.com/max/1600/1*pk05QGzoWhCgRiiFbz-oKQ.png)


The overall performance of a classifier, summarized over all possible tresholds is given by the Area under the Curve (AUC). An AUC of 0.95 is very good. Hence, we expect a classifier that performs no better than chance to have an AUC of 0.5. An excellent model has AUC near to the 1 which means it has good measure of separability.

If AUC is 0.5 the model has no discrimination capacity to distinguish between positive class and negative class.

## Quadratic Discriminant Analysis
LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class specific mean vector and a covariance matrix that is common to all K classes. 

Unlike LDA, QDA assumes that each class has its own covariance matrix. 

Why does it matter weather we assume that the K classes share a common covariance matrix? The answer is the bias-variance trade-off. When there is 1 covariance matrix and $p$ predictors then we estimate $\frac{p(p+1)}{2}$ parameters. If we estimate - like with QDA - a seperate covariance matrix for each predictor-class we get $Kp(p+1)/2$ parameters.

By assuming that there is a common covariance-matrix, we assume that x is linear and so there is lower variance. 

LDA is better if there are less training observations and QDA is better if there are lots of training observations.




# >>> Stopped at 153